{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fd4fe87",
   "metadata": {},
   "source": [
    "# Fake News Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e1e778",
   "metadata": {},
   "source": [
    "A fake news are those storeies that are fake, manupulated, no solid proof or not from reliable sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25f4b815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45e2f49",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee22ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = pd.read_csv(\"Fake.csv\")\n",
    "true = pd.read_csv(\"True.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979963cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01608a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "true.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e16ca2",
   "metadata": {},
   "source": [
    "# Data cleaning and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983d426b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add flag to track fake and real\n",
    "fake['target'] = 'fake'\n",
    "true['target'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c810e755",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce14390",
   "metadata": {},
   "outputs": [],
   "source": [
    "true.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec77f259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate dataframes\n",
    "data = pd.concat([fake, true]).reset_index(drop = True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57da2ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c2a6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da47b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "from sklearn.utils import shuffle\n",
    "data = shuffle(data)\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6960ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47a9eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b61e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the date \n",
    "data.drop([\"date\"],axis=1,inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0585ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the title\n",
    "data.drop([\"title\"],axis=1,inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb00b3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lowercase\n",
    "\n",
    "data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b452c894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "\n",
    "import string\n",
    "\n",
    "def punctuation_removal(text):\n",
    "    all_list = [char for char in text if char not in string.punctuation]\n",
    "    clean_str = ''.join(all_list)\n",
    "    return clean_str\n",
    "\n",
    "data['text'] = data['text'].apply(punctuation_removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63a129b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e3e6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stopwords\n",
    "import nltk #natural language tool kit\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "#stopword= and, or, a....\n",
    "data['text'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe11513",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b17211e",
   "metadata": {},
   "source": [
    "# Basic data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef54e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many articles per subject?\n",
    "print(data.groupby(['subject'])['text'].count())\n",
    "data.groupby(['subject'])['text'].count().plot(kind=\"bar\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fce8c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many fake and real articles?\n",
    "print(data.groupby(['target'])['text'].count())\n",
    "data.groupby(['target'])['text'].count().plot(kind=\"bar\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00f4f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word cloud for fake news\\\n",
    "#frequecy of words which are apppearing most\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "fake_data = data[data[\"target\"] == \"fake\"]\n",
    "all_words = ' '.join([text for text in fake_data.text])\n",
    "\n",
    "wordcloud = WordCloud(width= 800, height= 500,\n",
    "                          max_font_size = 110,\n",
    "                          collocations = False).generate(all_words)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8602d6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word cloud for real news\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "real_data = data[data[\"target\"] == \"true\"]\n",
    "all_words = ' '.join([text for text in fake_data.text])\n",
    "\n",
    "wordcloud = WordCloud(width= 800, height= 500,\n",
    "                          max_font_size = 110,\n",
    "                          collocations = False).generate(all_words)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f506262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequent words counter   \n",
    "from nltk import tokenize\n",
    "\n",
    "token_space = tokenize.WhitespaceTokenizer()\n",
    "\n",
    "def counter(text, column_text, quantity):\n",
    "    all_words = ' '.join([text for text in text[column_text]])\n",
    "    token_phrase = token_space.tokenize(all_words)\n",
    "    frequency = nltk.FreqDist(token_phrase)\n",
    "    df_frequency = pd.DataFrame({\"Word\": list(frequency.keys()),\n",
    "                                   \"Frequency\": list(frequency.values())})\n",
    "    df_frequency = df_frequency.nlargest(columns = \"Frequency\", n = quantity)\n",
    "    plt.figure(figsize=(12,8))\n",
    "    ax = sns.barplot(data = df_frequency, x = \"Word\", y = \"Frequency\", color = 'blue')\n",
    "    ax.set(ylabel = \"Count\")\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c1a9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequent words in fake news\n",
    "counter(data[data[\"target\"] == \"fake\"], \"text\", 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73191626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequent words in real news\n",
    "counter(data[data[\"target\"] == \"true\"], \"text\", 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3834b4db",
   "metadata": {},
   "source": [
    "Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67534fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the confusion matrix\n",
    "from sklearn import metrics\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82494162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train,X_test,y_train,y_test = train_test_split(data['text'], data.target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a680c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031ae82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38e8f26",
   "metadata": {},
   "source": [
    "# Decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a3dfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Vectorizing and applying TF-IDF\n",
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', DecisionTreeClassifier(criterion= 'entropy',\n",
    "                                           max_depth = 20, \n",
    "                                           splitter='best', \n",
    "                                           random_state=42))])\n",
    "# Fitting the model\n",
    "model = pipe.fit(X_train, y_train)\n",
    "\n",
    "# Accuracy\n",
    "prediction = model.predict(X_test)\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796af800",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_test, prediction)\n",
    "plot_confusion_matrix(cm, classes=['Fake', 'Real'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38340773",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
